---
title: "Reverse-engineering Claude's sandbox, then building my own"
description: "How Anthropic sandboxes Claude (spoiler: gVisor), and what I learned building a similar system for AI agents."
pubDate: 2025-11-28T18:00:00
tags: ['agents', 'infrastructure', 'security']
---

import Terminal from '../../components/Terminal.astro';
import Browser from '../../components/Browser.astro';
import ClaudeChat from '../../components/ClaudeChat.astro';

A few weeks ago, Anthropic gave Claude filesystem access. If you've used claude.ai recently, you've seen it - Claude can now write files, run Python, execute shell commands.

This wasn't just a feature. It was a bet on how agents should interact with the world.

If you're building an agent, you have two paths. **Path one: tools.** Want the agent to query a database? Build a tool. Search logs? Another tool. Transform data? Tool. Each one needs a schema, validation, error handling. Five actions means five tools. It doesn't scale.

**Path two: give it a terminal.** A bash shell is a meta-tool. One interface, infinite capability. The agent inherits everything the OS offers - Python, grep, awk, curl, the entire unix toolkit. Training data is abundant. The mental model is universal.

Anthropic chose path two. But if you give an agent unlimited OS access, you have a problem: containment. The agent can run arbitrary code. That code might be malicious, buggy, or just resource-hungry.

I was building an agent backend and needed to solve this same problem. Before writing any code, I wanted to see how Anthropic does it.

## Peeking inside Claude's sandbox

Here's the thing about reverse-engineering Claude's sandbox: Claude is the best tool for the job. I can just ask it to inspect its own environment.

<Browser url="claude.ai" title="Claude">
  <ClaudeChat
    initials="ML"
    maxHeight="500px"
    messages={[
      { role: 'human', text: 'What kernel are you running on? Run uname -a' },
      { role: 'assistant', content: [
        "I'll check what kernel I'm running on.",
        { tool: 'Check kernel version', command: 'uname -a', output: 'Linux runsc 4.4.0 #1 SMP Sun Jan 10 15:06:54 PST 2016 x86_64 GNU/Linux' },
        "Interesting - that runsc kernel and the 2016 date is gVisor's signature. I'm running in a gVisor sandbox, not a regular container or VM."
      ]},
      { role: 'human', text: 'What does your filesystem look like? Show me the mounts and root directory.' },
      { role: 'assistant', content: [
        { tool: 'List filesystem and mounts', command: 'ls -la / && mount', output: `drwxr-xr-x  root root     /
drwxr-xr-x  root root     bin
drwxr-xr-x  root root     etc
drwxr-xr-x  root root     home
drwxr-xr-x  root root     lib
drwxrwxrwx  sandbox sandbox  mnt
drwxr-xr-x  root root     usr
drwxrwxrwx  sandbox sandbox  workspace

none on / type overlay (rw,relatime,...)
/mnt/user-data/uploads type 9p (ro,relatime,...)
/mnt/user-data/outputs type 9p (rw,relatime,...)
/mnt/skills type 9p (ro,relatime,...)` },
        "The mounts use 9p - that's Plan 9's filesystem protocol. Uploads and skills are read-only, outputs is writable. I can read what you give me, write results to a specific location, and nothing else."
      ]},
      { role: 'human', text: 'How big is your image? What tools do you have?' },
      { role: 'assistant', content: [
        { tool: 'Check disk usage and available tools', command: 'du -sh / && du -sh /usr /home/claude /opt/pw-browsers /lib/jvm /lib/libreoffice && which ffmpeg convert pdflatex node', output: `7.3G    /
4.8G    /usr
3.0G    /home/claude
924M    /opt/pw-browsers
193M    /lib/jvm
307M    /lib/libreoffice

/bin/ffmpeg
/bin/convert
/bin/pdflatex
/bin/node` },
        "7.3GB total. I've got ffmpeg, ImageMagick, a full LaTeX distribution, Playwright browsers for web automation, LibreOffice for documents. Pretty much anything for file processing."
      ]}
    ]}
  />
</Browser>

A few things jumped out from this conversation:

**gVisor, not Firecracker.** The `runsc` kernel is gVisor's fingerprint. Anthropic chose userspace isolation over microVMs.

**9p mounts for file sharing.** Plan 9's filesystem protocol. The permission model is explicit: uploads and skills are read-only, outputs is writable.

**7.3GB image with everything.** ffmpeg, ImageMagick, LaTeX, Playwright, LibreOffice. Claude can process basically any file format.

**Non-root by default.** The `sandbox` user owns the writable directories.

Here's the breakdown of what's eating space:

| Path | Size | What it is |
|------|------|------------|
| `/usr` | 4.8GB | Binaries, libraries, shared data |
| `/home/claude` | 3.0GB | Working directory |
| `/opt/pw-browsers` | 924MB | Playwright browsers (headless Chrome/Firefox) |
| `/lib/jvm` | 193MB | Java runtime |
| `/lib/libreoffice` | 307MB | Document generation |

The tooling is extensive: ffmpeg for video processing, ImageMagick for images, a full LaTeX distribution, Playwright for browser automation, LibreOffice for documents. Claude can process basically any file format.

For my use case, I don't need most of this. A minimal agent sandbox can be ~200MB instead of 7GB.

So Anthropic chose gVisor for isolation, and a kitchen-sink image for capability. I'd copy the first decision but not the second.

## The options

**Firecracker** is what AWS uses for Lambda. MicroVMs that boot in ~125ms with ~5MB memory overhead. True VM-level isolation. The catch: it needs direct KVM access. Standard Kubernetes nodes are themselves VMs - Firecracker won't run inside them without bare metal instances. Operationally complex.

**gVisor** intercepts syscalls in userspace. Your container gets its own "kernel" - really a Go program pretending to be a kernel. It works anywhere Docker runs. Google uses this for Cloud Run and GKE Sandbox. Simpler to operate, slightly more syscall overhead.

**Plain Docker** shares the kernel with the host. Container escapes are rare but real. For untrusted code, that's not enough.

Anthropic chose gVisor. So did I.

## The sandbox image

First, what goes in the container:

```dockerfile
FROM python:3.13-slim-bookworm

RUN apt-get update && apt-get install -y --no-install-recommends \
    coreutils grep sed gawk findutils \
    curl wget git jq tree vim-tiny less procps \
    && rm -rf /var/lib/apt/lists/*

RUN mkdir -p /mnt/user-data/uploads \
             /mnt/user-data/outputs \
             /workspace

WORKDIR /workspace

RUN useradd -m -s /bin/bash sandbox && \
    chown -R sandbox:sandbox /mnt/user-data /workspace

USER sandbox

CMD ["tail", "-f", "/dev/null"]
```

Python, standard unix utils, a non-root user, and a directory structure that mirrors what I saw in Claude's environment. The `tail -f /dev/null` keeps the container alive without doing anything.

## Container lifecycle

Three options for when containers live and die:

**Pre-warmed pool**: Keep N containers running idle, grab one when needed. ~10-50ms latency. But you're managing a pool, handling assignment, dealing with cleanup. Complex.

**Per-execution**: New container for each command. Simplest code. ~600ms-1.2s cold start every time. Too slow.

**Session-scoped**: Container lives for the user session. Cold start once, then instant for every subsequent execution.

I went with session-scoped. The initial cold start (~800ms) hides behind LLM inference anyway - users are already waiting for the agent to think. By the time it responds, the container is warm.

```python
class SandboxManager:
    def __init__(self, image_name: str, runtime: str = "runsc", storage_path: Path = None):
        self.client = docker.from_env()
        self.image_name = image_name
        self.runtime = runtime
        self.storage_path = storage_path
        self.sessions: dict[str, SandboxSession] = {}

    async def create_session(self, session_id: str, tenant_id: str, network_enabled: bool = False):
        # Create tenant storage directories
        tenant_dir = self.storage_path / tenant_id
        (tenant_dir / "workspace").mkdir(parents=True, exist_ok=True)
        (tenant_dir / "outputs").mkdir(parents=True, exist_ok=True)

        container = self.client.containers.run(
            self.image_name,
            detach=True,
            name=f"sandbox-{session_id[:8]}",
            runtime=self.runtime,
            network_mode="bridge" if network_enabled else "none",
            mem_limit="512m",
            cpu_period=100000,
            cpu_quota=50000,
            security_opt=["no-new-privileges"],
            volumes={
                str(tenant_dir / "workspace"): {"bind": "/workspace", "mode": "rw"},
                str(tenant_dir / "outputs"): {"bind": "/mnt/user-data/outputs", "mode": "rw"},
            },
        )

        self.sessions[session_id] = SandboxSession(session_id, container, tenant_id)
        return container
```

## Defense in depth

Six layers of isolation:

**gVisor runtime** - Syscall interception, separate kernel. Even if code escapes the container, it hits gVisor's userspace kernel, not the host.

**No network by default** - `network_mode: none`. The sandbox can't phone home, can't exfiltrate data, can't download malware.

**Non-root user** - Everything runs as `sandbox`, not root. Limits what the process can touch.

**no-new-privileges** - Can't escalate via setuid binaries. Even if the agent finds a setuid binary, it can't use it to gain privileges.

**Resource limits** - 512MB memory, 50% of one CPU. A runaway process can't starve the host or other tenants.

**Path validation** - Only `/workspace` and `/mnt/user-data/outputs` are writable:

```python
async def write_file(self, session_id: str, path: str, content: str) -> tuple[bool, str]:
    if path.startswith("/"):
        full_path = path
    else:
        full_path = f"/workspace/{path}"

    allowed_prefixes = ["/workspace", "/mnt/user-data/outputs"]
    if not any(full_path.startswith(p) for p in allowed_prefixes):
        return False, "Path not allowed"

    # Write via container exec...
```

## The pip problem

No network means no `pip install`. But agents often need packages.

I considered a few approaches:

**Bake everything into the image.** Inflexible - you can't predict what an agent will need.

**Temporary network access.** Connect network, install, disconnect. Sounds clever, but Docker doesn't let you change `network_mode` on a running container.

**Network at session creation.** The caller decides upfront whether this sandbox needs internet access.

I went with option three. Not ideal - you have to know at session creation time whether you'll need pip. But it's simple and explicit.

```python
async def pip_install(self, session_id: str, packages: list[str]) -> ExecResponse:
    session = self.sessions.get(session_id)

    if not session.network_enabled:
        return ExecResponse(exit_code=-1, stderr="Network not enabled for this session")

    # Sanitize package names - block path traversal
    for pkg in packages:
        if not all(c.isalnum() or c in "-_.[]<>=!," for c in pkg):
            return ExecResponse(exit_code=-1, stderr=f"Invalid package name: {pkg}")

    packages_str = " ".join(f"'{p}'" for p in packages)
    return await self.exec_command(session_id, f"pip install --user {packages_str}")
```

The sanitization matters. A malicious agent shouldn't be able to sneak `pip install ../../etc/passwd` past you.

## Streaming output

Users want to see output as it happens, not wait for completion. Docker's exec API supports streaming:

```python
async def exec_stream(self, session_id: str, command: str, workdir: str = "/workspace"):
    session = self.sessions.get(session_id)
    container = session.container

    exec_instance = container.client.api.exec_create(
        container.id,
        cmd=["bash", "-c", command],
        workdir=workdir,
        stdout=True,
        stderr=True,
    )

    output = container.client.api.exec_start(exec_instance["Id"], stream=True, demux=True)

    for stdout_chunk, stderr_chunk in output:
        if stdout_chunk:
            yield {"type": "stdout", "data": stdout_chunk.decode("utf-8")}
        if stderr_chunk:
            yield {"type": "stderr", "data": stderr_chunk.decode("utf-8")}

    exec_info = container.client.api.exec_inspect(exec_instance["Id"])
    yield {"type": "exit", "code": exec_info["ExitCode"]}
```

The `demux=True` separates stdout and stderr. Without it, you get interleaved output and can't tell which is which.

## What it looks like from inside

<Terminal title="sandbox-abc123" maxHeight="none">
  <div class="sandbox-demo">
    <div class="cmd-line">
      <span class="prompt">$</span>
      <span class="cmd">uname -r</span>
    </div>
    <div class="output">4.4.0 runsc</div>
    <div class="annotation">gVisor, not host kernel</div>

    <div class="cmd-line">
      <span class="prompt">$</span>
      <span class="cmd">whoami</span>
    </div>
    <div class="output">sandbox</div>
    <div class="annotation">non-root user</div>

    <div class="cmd-line">
      <span class="prompt">$</span>
      <span class="cmd">curl https://example.com</span>
    </div>
    <div class="output error">curl: (6) Could not resolve host</div>
    <div class="annotation">no network</div>

    <div class="cmd-line">
      <span class="prompt">$</span>
      <span class="cmd">cat /etc/shadow</span>
    </div>
    <div class="output error">cat: /etc/shadow: Permission denied</div>
    <div class="annotation">no privilege escalation</div>

    <div class="cmd-line">
      <span class="prompt">$</span>
      <span class="cmd">ls /</span>
    </div>
    <div class="output">workspace  mnt</div>
    <div class="annotation">minimal filesystem</div>

    <div class="mount-info">
      <span class="arrow">↓</span>
      <span class="mount-text">/workspace mounts to /data/tenants/{`{id}`}/workspace on host</span>
    </div>
  </div>
</Terminal>

<style>{`
  .sandbox-demo {
    font-family: "SF Mono", Consolas, "Liberation Mono", Menlo, monospace;
    font-size: 13px;
    line-height: 1.6;
  }
  .cmd-line {
    margin-top: 8px;
  }
  .cmd-line:first-child {
    margin-top: 0;
  }
  .prompt {
    color: #4ec9b0;
    margin-right: 8px;
  }
  .cmd {
    color: #e0e0e0;
  }
  .output {
    color: #9cdcfe;
    margin-left: 16px;
  }
  .output.error {
    color: #f48771;
  }
  .annotation {
    color: #6a9955;
    font-style: italic;
    margin-left: 16px;
    font-size: 12px;
  }
  .annotation::before {
    content: "← ";
    color: #555;
  }
  .mount-info {
    margin-top: 16px;
    padding-top: 12px;
    border-top: 1px solid #333;
    color: #888;
    font-size: 12px;
  }
  .arrow {
    color: #569cd6;
    margin-right: 8px;
  }
`}</style>

## Trade-offs I accepted

**No container pooling.** Pre-warmed pools give you ~10-50ms latency instead of ~800ms. But session-scoped is simpler and the cold start hides behind LLM inference. I'll add pooling when latency actually becomes a problem.

**No snapshot/restore.** Firecracker can snapshot a running VM and restore in ~5ms. gVisor doesn't support this. If I ever need that performance, I'll revisit Firecracker and accept the operational complexity.

**Network is all-or-nothing at session creation.** Can't toggle it mid-session. Agents that might need pip have to opt in upfront, which means they have network access even when not installing packages. Not ideal.

**gVisor's syscall overhead.** Some workloads see 2-10x slowdown on syscall-heavy operations. For "run Python scripts and shell commands" this is negligible. For high-frequency I/O, you'd notice.

**No GPU support.** gVisor has experimental GPU passthrough, but I haven't needed it. When I do, this gets more complicated.

## The punchline

Firecracker is technically superior. Faster boot, true VM isolation, snapshot/restore. But it requires KVM access, which means bare metal or nested virtualization. For most teams running on standard cloud infrastructure, that's a non-starter.

gVisor is the practical choice. It works in standard Kubernetes, standard Docker, anywhere containers run. Google trusts it for Cloud Run. Anthropic trusts it for Claude. The isolation is real - you're running against a userspace kernel, not the host.

The pattern: gVisor runtime, session-scoped containers, defense in depth, network off by default. Simple enough to build in a weekend, robust enough to run in production.

If you're building agents that execute code, you need something like this. The alternative - running untrusted code on your host - is not an option.
